{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1867980e-4643-4843-b654-abc5521e5b06",
   "metadata": {},
   "source": [
    "# Object and Gestures Detection Model\n",
    "\n",
    "This project involves creating an Object and Gesture Detection Model as part of a school assignment. The goal is to develop a system that can accurately detect objects and recognize gestures made by a person, displaying the results in real-time. The project leverages computer vision and deep learning techniques to achieve robust and reliable detection and recognition.\n",
    "\n",
    "## Project Overview\n",
    "\n",
    "### Objective\n",
    "The primary objective of this project is to design and implement a model that can:\n",
    "\n",
    "1. **Detect Objects**: Identify and classify various objects shown by a person.\n",
    "2. **Recognize Gestures**: Interpret specific hand gestures made by a person.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47a90631-8c76-43f8-86d6-b73c4a87043d",
   "metadata": {},
   "source": [
    "### Imports\n",
    "<b>cv:</b> main import and the heart of this project it serves the bulid in Neural engine in our project <br>\n",
    "<b>matpotlib:</b> To view video in detection period or images<br>\n",
    "<b>os:</b> To write and read file smoothly<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3b5ede48-9816-44ca-9197-e7277dd94c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 as cv \n",
    "import matplotlib.pyplot as plt \n",
    "import os "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a68d7720-704f-4db2-a910-e7980811bd68",
   "metadata": {},
   "source": [
    "### File and model initlizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b74033aa-8819-4a96-bd64-afa0c976df28",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_file = 'resources/ssd_mobilenet_v3_large_coco_2020_01_14.pbtxt'\n",
    "frozen_model = 'resources/frozen_inference_graph.pb'\n",
    "img_folder = \"data/images\"\n",
    "video_folder = \"data/videos\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e5e7825-71ce-4e0b-9c6a-80db7661ab28",
   "metadata": {},
   "source": [
    "### List of images and videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e3db1c96-cab1-452f-81f0-2d7f85c18c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ren_dir_file(dir_folder_index):\n",
    "    if(dir_folder_index == 0):\n",
    "        files = files_and_dirs = os.listdir(img_folder) \n",
    "        files = [img_folder+'/'+i for i in files]\n",
    "        return files\n",
    "    elif(dir_folder_index == 1):   \n",
    "        files = files_and_dirs = os.listdir(video_folder) \n",
    "        files = [video_folder+'/'+i for i in files]\n",
    "        return files\n",
    "    else:\n",
    "        print(\"Invalid Index\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2be8fa9-889e-458d-a252-7f77078252fb",
   "metadata": {},
   "source": [
    "### Building model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b250c738-e93b-469e-b6c7-80bfd57b86da",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = cv.dnn_DetectionModel(frozen_model, config_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76aa810f-b3db-4c40-985b-2243c55d835a",
   "metadata": {},
   "source": [
    "### Fetching data from .txt file in the directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "93b5f15f-95cf-4135-bea6-109f56401174",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = []\n",
    "with open('resources/labels.txt','r') as f: # path is defined \n",
    "    for i in f:\n",
    "        labels.append(i.split(\"\\n\")[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1feea387-098b-4482-b193-c26f97241e18",
   "metadata": {},
   "source": [
    "### Setting frame of the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9601e7a8-ca74-4ed9-b90f-bc023e967ffb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "< cv2.dnn.Model 00000267BA0F0C90>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.setInputSize(320,320)\n",
    "model.setInputScale(1.0/127.5)\n",
    "model.setInputMean((127.5,127,5,127.5))\n",
    "model.setInputSwapRB(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba52f46d-4b20-4927-ac1c-9d5a6afa199b",
   "metadata": {},
   "source": [
    "### Main Function to detect Objects from Images\n",
    "<i>To set path of images</i><br>\n",
    "<b>os module:</b> It is used here to get list of images and to render it automatically.<br>\n",
    "<b>matplotlib:</b> To preview image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a29fd5e6-ca33-40e0-9d74-dd81a9179889",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/images/HQ_street.jpeg\n"
     ]
    }
   ],
   "source": [
    "f = ren_dir_file(0)\n",
    "print(f[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc47e05-cba8-46bc-9db7-d00f93df3b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "        img = cv.imread(f[0])\n",
    "        ClassIndex,confidence,bbox = model.detect(img,confThreshold = 0.5)\n",
    "        font_scale = 3\n",
    "        font= cv.FONT_HERSHEY_PLAIN\n",
    "        for ClassInd, conf, boxes in zip(ClassIndex.flatten(), confidence.flatten(), bbox):\n",
    "            cv.rectangle(img, boxes, (255,0,0),2)\n",
    "            cv.putText(img, labels[ClassInd-1], (boxes[0]+8, boxes[1]+32), font, font_scale, color=(0,255,0), thickness=2)\n",
    "        plt.imshow(cv.cvtColor(img,cv.COLOR_BGR2RGB))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "66abf7b5-1699-408a-a4ed-be9eeb7ea6bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def detect_objects_from_images(imgIndex):\n",
    "#     img = cv.imread(f[0])\n",
    "#     ClassIndex,confidence,bbox = model.detect(img,confThreshold = 0.5)\n",
    "#     font_scale = 3\n",
    "#     font= cv.FONT_HERSHEY_PLAIN\n",
    "#     for ClassInd, conf, boxes in zip(ClassIndex.flatten(), confidence.flatten(), bbox):\n",
    "#         cv.rectangle(img, boxes, (255,0,0),2)\n",
    "#         cv.putText(img, labels[ClassInd-1], (boxes[0]+8, boxes[1]+32), font, font_scale, color=(0,255,0), thickness=2)\n",
    "#     plt.imshow(cv.cvtColor(img,cv.COLOR_BGR2RGB))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "0d96b6ce-1f09-40b2-bfca-ca7280b08e6b",
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "OpenCV(4.10.0) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\dnn\\src\\model.cpp:104: error: (-201:Incorrect size of input array) Input size not specified in function 'cv::dnn::dnn4_v20240521::Model::Impl::processFrame'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[50], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mdetect_objects_from_images\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[49], line 3\u001b[0m, in \u001b[0;36mdetect_objects_from_images\u001b[1;34m(imgIndex)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdetect_objects_from_images\u001b[39m(imgIndex):\n\u001b[0;32m      2\u001b[0m     img \u001b[38;5;241m=\u001b[39m cv\u001b[38;5;241m.\u001b[39mimread(f[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m----> 3\u001b[0m     ClassIndex,confidence,bbox \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43mconfThreshold\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m     font_scale \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3\u001b[39m\n\u001b[0;32m      5\u001b[0m     font\u001b[38;5;241m=\u001b[39m cv\u001b[38;5;241m.\u001b[39mFONT_HERSHEY_PLAIN\n",
      "\u001b[1;31merror\u001b[0m: OpenCV(4.10.0) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\dnn\\src\\model.cpp:104: error: (-201:Incorrect size of input array) Input size not specified in function 'cv::dnn::dnn4_v20240521::Model::Impl::processFrame'\n"
     ]
    }
   ],
   "source": [
    "detect_objects_from_images(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "984ab7b3-8f06-4101-b913-ad7877489a9b",
   "metadata": {},
   "source": [
    "### Main Function to detect Objects from Videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0a35b064-72d7-4f2e-942e-ad1cde07e741",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = ren_dir_file(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cb1eb5a8-bada-4c85-8c0c-deb39c953ee4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data/videos/street.mp4']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3cee2f8-7a0a-4ad2-8458-b01ac195bfee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
